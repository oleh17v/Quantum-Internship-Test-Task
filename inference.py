# -*- coding: utf-8 -*-
"""inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GI8ngs-mpMcHWb8omYwxxXArDPY4tZOR
"""

import torch
from transformers import BertTokenizerFast, BertForTokenClassification

def setup_model(model_name='bert-base-uncased', num_labels=3):
    """
    Set up the tokenizer and model for token classification.
    """
    tokenizer = BertTokenizerFast.from_pretrained(model_name, add_prefix_space=True)
    model = BertForTokenClassification.from_pretrained(
        model_name, num_labels=num_labels,
        output_attentions=False, output_hidden_states=False
    )
    return tokenizer, model

def load_model(model_path, model):
    """
    Load a trained model checkpoint.
    """
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint)
    model.eval()
    print(f"Model loaded from {model_path}")
    return model

def preprocess_sentence(sentence, tokenizer, max_length=128):
    """
    Preprocess the input sentence by tokenizing and adding padding.
    """
    tokenized_inputs = tokenizer(
        sentence, padding="max_length", truncation=True,
        return_tensors="pt", max_length=max_length
    )

    input_ids = tokenized_inputs["input_ids"]
    attention_masks = tokenized_inputs["attention_mask"]

    return input_ids, attention_masks

def predict(texts, tokenizer, model, device):
    """
    Perform prediction for multiple sentences and return results.
    """
    all_results = []
    for text in texts:
        # Preprocess the sentence
        input_ids, attention_masks = preprocess_sentence(text, tokenizer)

        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)

        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_masks)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=2).squeeze().cpu().numpy()

        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().numpy())
        labels = ['O', 'B-MOUNTAIN', 'I-MOUNTAIN']
        predicted_labels = [labels[label_id] for label_id in predictions]

        # Collect results for each sentence, excluding [PAD] tokens
        results = [{"token": token, "label": label} for token, label in zip(tokens, predicted_labels) if token not in ['[PAD]', '[CLS]', '[SEP]']]
        all_results.append(results)

    return all_results

def print_predictions(all_results):
    """
    Print the token predictions in a readable format.
    """
    for results in all_results:
        print("\nPrediction for sentence:")
        for prediction in results:
            print(f"Token: {prediction['token']}, Label: {prediction['label']}")

if __name__ == "__main__":

    tokenizer, model = setup_model(model_name='bert-base-uncased')

    model = load_model('/content/models/model.pth', model)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    texts = [
    "We visited the Sierra Nevada during our road trip.",
    "Mount Denali, the highest peak in North America, is in Alaska.",
    "Mount Aconcagua is the tallest peak in South America.",
    ]

    predictions = predict(texts, tokenizer, model, device)

    print_predictions(predictions)