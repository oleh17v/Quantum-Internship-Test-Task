# -*- coding: utf-8 -*-
"""Task_1_NER_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10PMmpwTJEXGyoT4BB4aWc0jUGt2iZ0vx
"""

import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset
from transformers import BertTokenizerFast, BertForTokenClassification, get_scheduler, AdamW
from tqdm import tqdm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import zipfile

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



with zipfile.ZipFile('/content/generated-mountain-entity-recognition-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('/content')

df = pd.read_csv("/content/mountain_sentences_ner.csv")

df.fillna({'entity': 'none', 'start_idx': -1, 'end_idx': -1}, inplace=True)
df.head()

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

def preprocess_ner_data_with_tokenizer(df, tokenizer, max_seq_length=128):
    """
    Preprocesses the dataset for NER by tokenizing, creating labels, and padding.

    Args:
        df (pd.DataFrame): Input dataframe with columns
                           ['original_sentence', 'new_sentence', 'entity', 'start_idx', 'end_idx'].
        tokenizer (transformers.BertTokenizerFast): Tokenizer for the model.
        max_seq_length (int): Maximum sequence length for tokenization.

    Returns:
        TensorDataset: A PyTorch dataset containing input IDs, attention masks, and labels.
    """
    input_ids, attention_masks, label_ids = [], [], []

    for _, row in df.iterrows():
        sentence = row['new_sentence']
        entity = row['entity']
        start = row['start_idx']
        end = row['end_idx']

        # Tokenize the sentence
        tokenized = tokenizer(sentence,
                              padding="max_length",
                              truncation=True,
                              return_offsets_mapping=True,
                              max_length=max_seq_length,
                              return_tensors="pt")

        # Get token offset mappings
        offset_mapping = tokenized.pop("offset_mapping")[0]
        labels = [label_to_id('O')] * len(offset_mapping)

        # Label assignment based on entity spans
        for idx, (start_offset, end_offset) in enumerate(offset_mapping):
            if start_offset >= start and end_offset <= end:
                if labels[idx - 1] == label_to_id('O') if idx > 0 else True:  # First entity token
                    labels[idx] = label_to_id('B-MOUNTAIN')
                else:
                    labels[idx] = label_to_id('I-MOUNTAIN')

        input_ids.append(tokenized['input_ids'][0])
        attention_masks.append(tokenized['attention_mask'][0])

        # Padding labels to match max_seq_length
        labels = labels + [label_to_id('O')] * (max_seq_length - len(labels))
        label_ids.append(torch.tensor(labels[:max_seq_length]))

    # Stack tensors and create TensorDataset
    input_ids = torch.stack(input_ids)
    attention_masks = torch.stack(attention_masks)
    label_ids = torch.stack(label_ids)

    return TensorDataset(input_ids, attention_masks, label_ids)

def label_to_id(label):
    if label == 'B-MOUNTAIN':
        return 1
    elif label == 'I-MOUNTAIN':
        return 2
    else:
        return 0  # 'O' for other tokens

    # Stack tensors and create TensorDataset
    input_ids = torch.stack(input_ids)
    attention_masks = torch.stack(attention_masks)
    label_ids = torch.stack(label_ids)

    return TensorDataset(input_ids, attention_masks, label_ids)

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", add_prefix_space=True)
model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=3).to(device)
optimizer = AdamW(model.parameters(), lr=5e-5)

train_dataset = preprocess_ner_data_with_tokenizer(train_df, tokenizer, max_seq_length=128)
test_dataset = preprocess_ner_data_with_tokenizer(test_df, tokenizer, max_seq_length=128)

train_dataloader = DataLoader(train_dataset, batch_size=16)
test_dataloader = DataLoader(test_dataset, batch_size=16)

def train_and_test_model(model, train_loader, test_loader, optimizer, device):
    # Training Phase
    model.train()
    total_train_loss = 0
    all_preds = []
    all_labels = []

    for batch in train_loader:
        # Move batch data to device
        batch = tuple(b.to(device) for b in batch)
        b_input_ids, b_input_mask, b_labels = batch

        model.zero_grad()

        # Forward pass
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs.loss
        total_train_loss += loss.item()

        # Backward pass and optimizer step
        loss.backward()
        optimizer.step()

        # Collect predictions and true labels for accuracy calculation
        logits = outputs.logits.detach().cpu().numpy()
        label_ids = b_labels.cpu().numpy()
        all_preds.extend(logits.argmax(axis=-1).flatten())
        all_labels.extend(label_ids.flatten())

    avg_train_loss = total_train_loss / len(train_loader)
    train_accuracy = accuracy_score(all_labels, all_preds) * 100
    print(f"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%")

    # Evaluation Phase
    model.eval()
    total_test_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in test_loader:
            # Move batch data to device
            batch = tuple(b.to(device) for b in batch)
            b_input_ids, b_input_mask, b_labels = batch

            # Forward pass
            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)

            loss = outputs.loss
            total_test_loss += loss.item()

            # Collect predictions and true labels for accuracy calculation
            logits = outputs.logits.detach().cpu().numpy()
            label_ids = b_labels.cpu().numpy()
            all_preds.extend(logits.argmax(axis=-1).flatten())
            all_labels.extend(label_ids.flatten())

    avg_test_loss = total_test_loss / len(test_loader)
    test_accuracy = accuracy_score(all_labels, all_preds) * 100
    print(f"Test Loss: {avg_test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%")

epochs = 5
metrics = []

for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch + 1}\n")
    train_and_test_model(
        model=model,
        train_loader=train_dataloader,
        test_loader=test_dataloader,
        optimizer=optimizer,
        device=device
    )

from pathlib import Path

path = Path("models")
path.mkdir(parents=True, exist_ok=True)

model_name = "model.pth"

save = path / model_name

torch.save(obj = model.state_dict(), f=save)